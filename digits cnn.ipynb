{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f7009ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow.keras as keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Conv2D, MaxPool2D, Flatten, BatchNormalization\n",
    "from keras.backend import clear_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a056bd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c7d957",
   "metadata": {},
   "source": [
    "EDA was completed in previous notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "773c4067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set shape: (42000, 784)\n",
      "training labels shape: (42000,)\n"
     ]
    }
   ],
   "source": [
    "# Seperate x and y values for training set\n",
    "# Cast the values inside of the data frame as approrpiate data type\n",
    "y = train.label.values.astype('int32')\n",
    "x = train.drop(columns=['label']).values.astype('float32')\n",
    "print('training set shape: {}'.format(x.shape))\n",
    "print('training labels shape: {}'.format(y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b46406a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((42000, 28, 28, 1), (28000, 28, 28, 1))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize data and reshape to a 3-d array\n",
    "scale = np.max(x)\n",
    "x_norm = x/scale\n",
    "\n",
    "# Cast test dataframe into a numpy array\n",
    "test = test.values.astype('float32')\n",
    "test_norm = test/scale\n",
    "\n",
    "x_norm = x_norm.reshape(-1,28,28,1)\n",
    "test_norm = test_norm.reshape(-1,28,28,1)\n",
    "\n",
    "x_norm.shape, test_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "689db820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform categorical encoding on the training labels\n",
    "num_categories = 10\n",
    "y = keras.utils.to_categorical(y, num_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d918255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31500, 28, 28, 1) (31500, 10)\n",
      "(10500, 28, 28, 1) (10500, 10)\n"
     ]
    }
   ],
   "source": [
    "# Split the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_norm,y,random_state=69)\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0f6dacbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to build a cnn and return the untrained model\n",
    "def base_cnn():\n",
    "    model = Sequential()\n",
    "    # Input layer is conv layer with kernel size of 3\n",
    "    model.add(Conv2D(64, kernel_size=3, activation='relu', input_shape=(28,28,1))) # Expect input to be a b/w image 28x28 px\n",
    "    model.add(Conv2D(32, kernel_size=3, activation='relu')) # Hidden layer\n",
    "    # Flatten model from 2d to 1d\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(10, activation='softmax')) # Output layer\n",
    "    # Return the model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "70c1430d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a cnn model with batch normalization\n",
    "def batch_norm_cnn():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, kernel_size=3, activation='relu', input_shape=(28,28,1))) # Expect input to be a b/w image 28x28 px\n",
    "    # Recentre and re-scale inputs\n",
    "    model.add(BatchNormalization())\n",
    "    # Partition data into 2x2 fixed windows and take the max value from each section to create a new array\n",
    "    model.add(MaxPool2D((2, 2), strides=2, padding=\"same\"))\n",
    "    model.add(Conv2D(32, (3, 3), strides=1, padding=\"same\", activation=\"relu\")) # Hidden layer\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool2D((2, 2), strides=2, padding=\"same\"))\n",
    "    # Flatten the output to 1-d\n",
    "    model.add(Flatten())\n",
    "    # Dense layers for classification\n",
    "    model.add(Dense(units=10, activation=\"softmax\"))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bc7e0987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_2 (Conv2D)           (None, 26, 26, 64)        640       \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 24, 24, 32)        18464     \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 18432)             0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                184330    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 203,434\n",
      "Trainable params: 203,434\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create model and view summary\n",
    "base_cnn = base_cnn()\n",
    "base_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "991e49fb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "985/985 [==============================] - 6s 6ms/step - loss: 0.1845 - accuracy: 0.9443 - val_loss: 0.0888 - val_accuracy: 0.9728\n",
      "Epoch 2/10\n",
      "985/985 [==============================] - 6s 6ms/step - loss: 0.0597 - accuracy: 0.9819 - val_loss: 0.0690 - val_accuracy: 0.9794\n",
      "Epoch 3/10\n",
      "985/985 [==============================] - 6s 6ms/step - loss: 0.0394 - accuracy: 0.9873 - val_loss: 0.0705 - val_accuracy: 0.9792\n",
      "Epoch 4/10\n",
      "985/985 [==============================] - 6s 6ms/step - loss: 0.0271 - accuracy: 0.9916 - val_loss: 0.0721 - val_accuracy: 0.9805\n",
      "Epoch 5/10\n",
      "985/985 [==============================] - 6s 6ms/step - loss: 0.0192 - accuracy: 0.9938 - val_loss: 0.0618 - val_accuracy: 0.9842\n",
      "Epoch 6/10\n",
      "985/985 [==============================] - 6s 6ms/step - loss: 0.0139 - accuracy: 0.9954 - val_loss: 0.0707 - val_accuracy: 0.9836\n",
      "Epoch 7/10\n",
      "985/985 [==============================] - 6s 6ms/step - loss: 0.0117 - accuracy: 0.9960 - val_loss: 0.0767 - val_accuracy: 0.9828\n",
      "Epoch 8/10\n",
      "985/985 [==============================] - 6s 6ms/step - loss: 0.0073 - accuracy: 0.9975 - val_loss: 0.0727 - val_accuracy: 0.9850\n",
      "Epoch 9/10\n",
      "985/985 [==============================] - 6s 6ms/step - loss: 0.0073 - accuracy: 0.9977 - val_loss: 0.0969 - val_accuracy: 0.9805\n",
      "Epoch 10/10\n",
      "985/985 [==============================] - 6s 6ms/step - loss: 0.0064 - accuracy: 0.9974 - val_loss: 0.1172 - val_accuracy: 0.9798\n"
     ]
    }
   ],
   "source": [
    "# Compile and train\n",
    "base_cnn.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "cnn_hist = base_cnn.fit(\n",
    "    x_train, y_train, epochs=10, verbose=1, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8d815e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9967\n",
      "Validation Accuracy: 0.9798\n"
     ]
    }
   ],
   "source": [
    "# Print the metrics    \n",
    "loss,accuracy = base_cnn.evaluate(x_train,y_train,verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss,accuracy = base_cnn.evaluate(x_val, y_val, verbose=False)\n",
    "print(\"Validation Accuracy: {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "891949f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c1a85797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 26, 26, 64)        640       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 26, 26, 64)       256       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 13, 13, 64)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 13, 13, 32)        18464     \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 13, 13, 32)       128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 7, 7, 32)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1568)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                15690     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 35,178\n",
      "Trainable params: 34,986\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Load model with batch normalization and train it\n",
    "bn_cnn = batch_norm_cnn()\n",
    "bn_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1c6f228e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "985/985 [==============================] - 7s 7ms/step - loss: 0.1460 - accuracy: 0.9543 - val_loss: 0.0706 - val_accuracy: 0.9790\n",
      "Epoch 2/10\n",
      "985/985 [==============================] - 6s 6ms/step - loss: 0.0562 - accuracy: 0.9822 - val_loss: 0.0715 - val_accuracy: 0.9792\n",
      "Epoch 3/10\n",
      "985/985 [==============================] - 6s 6ms/step - loss: 0.0387 - accuracy: 0.9872 - val_loss: 0.1260 - val_accuracy: 0.9673\n",
      "Epoch 4/10\n",
      "985/985 [==============================] - 6s 7ms/step - loss: 0.0314 - accuracy: 0.9900 - val_loss: 0.0625 - val_accuracy: 0.9837\n",
      "Epoch 5/10\n",
      "985/985 [==============================] - 6s 7ms/step - loss: 0.0250 - accuracy: 0.9924 - val_loss: 0.0838 - val_accuracy: 0.9784\n",
      "Epoch 6/10\n",
      "985/985 [==============================] - 6s 6ms/step - loss: 0.0226 - accuracy: 0.9929 - val_loss: 0.0755 - val_accuracy: 0.9808\n",
      "Epoch 7/10\n",
      "985/985 [==============================] - 6s 6ms/step - loss: 0.0164 - accuracy: 0.9945 - val_loss: 0.0770 - val_accuracy: 0.9813\n",
      "Epoch 8/10\n",
      "985/985 [==============================] - 6s 6ms/step - loss: 0.0174 - accuracy: 0.9943 - val_loss: 0.0829 - val_accuracy: 0.9818\n",
      "Epoch 9/10\n",
      "985/985 [==============================] - 6s 6ms/step - loss: 0.0130 - accuracy: 0.9959 - val_loss: 0.0640 - val_accuracy: 0.9851\n",
      "Epoch 10/10\n",
      "985/985 [==============================] - 6s 7ms/step - loss: 0.0112 - accuracy: 0.9960 - val_loss: 0.0754 - val_accuracy: 0.9843\n"
     ]
    }
   ],
   "source": [
    "# Compile and train\n",
    "bn_cnn.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "bn_cnn_hist = bn_cnn.fit(\n",
    "    x_train, y_train, epochs=10, verbose=1, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1fedf3d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9966\n",
      "Validation Accuracy: 0.9843\n"
     ]
    }
   ],
   "source": [
    "# Print the metrics    \n",
    "loss,accuracy = bn_cnn.evaluate(x_train,y_train,verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss,accuracy = bn_cnn.evaluate(x_val, y_val, verbose=False)\n",
    "print(\"Validation Accuracy: {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0844fb42",
   "metadata": {},
   "source": [
    "The batch normalization and pooling layers helped improve the validation acc by about 1%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a24c255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get function to print predictions\n",
    "import sys\n",
    "sys.path.append('C://Users//Chris//Desktop//ml-env//scripts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "243f3ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from predictions import get_preds\n",
    "get_preds(base_cnn,test_norm,'submissions//base_cnn_preds.csv')\n",
    "get_preds(bn_cnn, test_norm, 'submissions//batch_norm_cnn_preds.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e892d79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
